import { useState, useRef, useCallback, useEffect } from 'react';
import { Button, Container, Badge } from 'react-bootstrap';
import { FaMicrophone, FaMicrophoneSlash, FaPhone, FaPhoneSlash } from 'react-icons/fa';
import 'bootstrap/dist/css/bootstrap.min.css';

function useSpeechRecognition() {
  const recognitionRef = useRef(null);
  const isSupported = 'SpeechRecognition' in window || 'webkitSpeechRecognition' in window;

  const startListening = useCallback((language, onResult, onEnd) => {
    if (!isSupported) return;
    const SpeechRecognition = window.SpeechRecognition || window.webkitSpeechRecognition;
    const recognition = new SpeechRecognition();
    recognition.continuous = false;
    recognition.interimResults = false;
    recognition.lang = {
      hindi: 'hi-IN',
      tamil: 'ta-IN',
      english: 'en-US'
    }[language] || 'hi-IN';

    recognition.onresult = (event) => {
      const transcript = event.results[0][0].transcript;
      onResult(transcript);
    };

    recognition.onend = () => {
      recognitionRef.current = null;
      onEnd?.();
    };

    recognition.onerror = (event) => {
      console.error('Speech recognition error:', event.error);
      recognitionRef.current = null;
      onEnd?.();
    };

    recognitionRef.current = recognition;
    recognition.start();
  }, [isSupported]);

  const stopListening = useCallback(() => {
    if (recognitionRef.current) {
      recognitionRef.current.stop();
      recognitionRef.current = null;
    }
  }, []);

  return { startListening, stopListening, isSupported };
}

export default function VoiceAssistant() {
  const [state, setState] = useState({
    isConnected: false,
    isListening: false,
    isSpeaking: false,
    isMuted: false,
    selectedLanguage: null,
    sessionTimer: 0,
    conversation: [],
    status: '‡§™‡•ç‡§∞‡§æ‡§∞‡§Ç‡§≠ ‡§ï‡§∞‡§®‡•á ‡§ï‡•á ‡§≤‡§ø‡§è ‡§§‡•à‡§Ø‡§æ‡§∞',
    statusType: 'disconnected'
  });

  const timerRef = useRef(null);
  const { startListening, stopListening, isSupported } = useSpeechRecognition();

  const updateStatus = useCallback((status, statusType) => {
    setState(prev => ({ ...prev, status, statusType }));
  }, []);

  const startTimer = useCallback(() => {
    let seconds = 0;
    timerRef.current = setInterval(() => {
      seconds++;
      setState(prev => ({ ...prev, sessionTimer: seconds }));
    }, 1000);
  }, []);

  const stopTimer = useCallback(() => {
    clearInterval(timerRef.current);
    timerRef.current = null;
    setState(prev => ({ ...prev, sessionTimer: 0 }));
  }, []);

  const formatTime = (seconds) => `${String(Math.floor(seconds / 60)).padStart(2, '0')}:${String(seconds % 60).padStart(2, '0')}`;

  const getStatusVariant = () => ({
    ready: 'success',
    listening: 'primary',
    speaking: 'warning',
    processing: 'info',
    error: 'danger',
    disconnected: 'secondary'
  }[state.statusType] || 'secondary');

  const speakText = useCallback((text, language = 'hindi') => {
    return new Promise((resolve) => {
      window.speechSynthesis.cancel();
      setState(prev => ({ ...prev, isSpeaking: true }));
      updateStatus('‡§¨‡•ã‡§≤ ‡§∞‡§π‡§æ ‡§π‡•Ç‡§Å...', 'speaking');

      const utterance = new SpeechSynthesisUtterance(text);
      utterance.lang = {
        hindi: 'hi-IN',
        tamil: 'ta-IN',
        english: 'en-US'
      }[language] || 'hi-IN';

      utterance.onend = () => {
        setState(prev => ({ ...prev, isSpeaking: false }));
        updateStatus('‡§∏‡•Å‡§®‡§®‡•á ‡§ï‡•á ‡§≤‡§ø‡§è ‡§§‡•à‡§Ø‡§æ‡§∞', 'ready');
        resolve();
      };

      utterance.onerror = () => {
        setState(prev => ({ ...prev, isSpeaking: false }));
        updateStatus('‡§¨‡•ã‡§≤‡§®‡•á ‡§Æ‡•á‡§Ç ‡§§‡•ç‡§∞‡•Å‡§ü‡§ø', 'error');
        resolve();
      };

      window.speechSynthesis.speak(utterance);
    });
  }, [updateStatus]);

  const detectLanguageFromSpeech = useCallback((text) => {
    const t = text.toLowerCase();
    if (t.includes('hindi') || t.includes('‡§π‡§ø‡§Ç‡§¶‡•Ä') || t.includes('‡§π‡§ø‡§®‡•ç‡§¶‡•Ä')) return 'hindi';
    if (t.includes('tamil') || t.includes('‡Æ§‡ÆÆ‡Æø‡Æ¥‡Øç') || t.includes('‡§§‡§Æ‡§ø‡§≤')) return 'tamil';
    if (t.includes('english') || t.includes('‡§Ö‡§Ç‡§ó‡•ç‡§∞‡•á‡§ú‡•Ä') || t.includes('‡§á‡§Ç‡§ó‡•ç‡§≤‡§ø‡§∂')) return 'english';
    return null;
  }, []);

  const startListeningLoop = useCallback(() => {
    if (!state.isConnected || state.isMuted || state.isSpeaking) return;
    setState(prev => ({ ...prev, isListening: true }));
    updateStatus('‡§∏‡•Å‡§® ‡§∞‡§π‡§æ ‡§π‡•Ç‡§Å...', 'listening');

    startListening(state.selectedLanguage || 'hindi', async (transcript) => {
      const lang = detectLanguageFromSpeech(transcript);
      if (!state.selectedLanguage && lang) {
        await speakText({
          hindi: '‡§π‡§ø‡§Ç‡§¶‡•Ä ‡§ö‡•Å‡§®‡•Ä ‡§ó‡§à‡•§ ‡§Ö‡§¨ ‡§Ü‡§™ ‡§™‡•ç‡§∞‡§∂‡•ç‡§® ‡§™‡•Ç‡§õ‡•á‡§Ç‡•§',
          tamil: '‡Æ§‡ÆÆ‡Æø‡Æ¥‡Øç ‡Æ§‡Øá‡Æ∞‡Øç‡Æ®‡Øç‡Æ§‡ØÜ‡Æü‡ØÅ‡Æï‡Øç‡Æï‡Æ™‡Øç‡Æ™‡Æü‡Øç‡Æü‡Æ§‡ØÅ. ‡Æâ‡Æô‡Øç‡Æï‡Æ≥‡Øç ‡Æï‡Øá‡Æ≥‡Øç‡Æµ‡Æø‡ÆØ‡Øà ‡Æï‡Øá‡Æ≥‡ØÅ‡Æô‡Øç‡Æï‡Æ≥‡Øç.',
          english: 'English selected. Please ask your question.'
        }[lang], lang);
        setState(prev => ({ ...prev, selectedLanguage: lang }));
        setTimeout(() => startListeningLoop(), 500);
        return;
      }
    }, () => {
      setState(prev => ({ ...prev, isListening: false }));
      if (state.isConnected && !state.isSpeaking) {
        setTimeout(() => startListeningLoop(), 1000);
      }
    });
  }, [state.isConnected, state.selectedLanguage, state.isSpeaking, state.isMuted, startListening, detectLanguageFromSpeech, speakText, updateStatus]);

  const requestMicrophonePermission = useCallback(async () => {
    try {
      const stream = await navigator.mediaDevices.getUserMedia({ audio: true });
      stream.getTracks().forEach(track => track.stop());
      return true;
    } catch (error) {
      console.error("Microphone permission denied:", error);
      updateStatus('‡§Æ‡§æ‡§á‡§ï‡•ç‡§∞‡•ã‡§´‡•ã‡§® ‡§ï‡•Ä ‡§Ö‡§®‡•Å‡§Æ‡§§‡§ø ‡§ö‡§æ‡§π‡§ø‡§è', 'error');
      return false;
    }
  }, [updateStatus]);

  const startSession = useCallback(async () => {
    if (!(await requestMicrophonePermission())) {
      alert('Please allow microphone access.');
      return;
    }
    setState(prev => ({ ...prev, isConnected: true }));
    updateStatus('‡§ï‡§®‡•á‡§ï‡•ç‡§ü ‡§π‡•ã ‡§∞‡§π‡§æ...', 'ready');
    startTimer();
    await speakText('‡§®‡§Æ‡§∏‡•ç‡§§‡•á! ‡§Æ‡•à‡§Ç ‡§®‡•ç‡§Ø‡§æ‡§Ø GPT ‡§π‡•Ç‡§Å‡•§ ‡§ï‡•É‡§™‡§Ø‡§æ ‡§≠‡§æ‡§∑‡§æ ‡§ö‡•Å‡§®‡•á‡§Ç - ‡§π‡§ø‡§Ç‡§¶‡•Ä, ‡§§‡§Æ‡§ø‡§≤ ‡§Ø‡§æ ‡§Ö‡§Ç‡§ó‡•ç‡§∞‡•á‡§ú‡•Ä ‡§ï‡§π‡•á‡§Ç‡•§', 'hindi');
    setTimeout(() => startListeningLoop(), 1000);
  }, [requestMicrophonePermission, updateStatus, startTimer, speakText, startListeningLoop]);

  const endSession = useCallback(() => {
    stopListening();
    window.speechSynthesis.cancel();
    stopTimer();
    setState(prev => ({ ...prev, isConnected: false, isListening: false, isSpeaking: false, selectedLanguage: null, conversation: [] }));
    updateStatus('‡§∏‡§§‡•ç‡§∞ ‡§∏‡§Æ‡§æ‡§™‡•ç‡§§', 'disconnected');
  }, [stopListening, stopTimer, updateStatus]);

  if (!isSupported) {
    return <Container className="p-4"><h4 className="text-danger">‚ö† Speech Recognition not supported in your browser</h4></Container>;
  }

  return (
    <Container className="p-4">
      <h2>üßë‚Äç‚öñÔ∏è ‡§®‡•ç‡§Ø‡§æ‡§Ø GPT</h2>
      <p>Status: <Badge bg={getStatusVariant()}>{state.status}</Badge></p>
      <p>Time: {formatTime(state.sessionTimer)}</p>
      <Button onClick={startSession} variant={state.isConnected ? "danger" : "success"}>
        {state.isConnected ? <FaPhoneSlash /> : <FaPhone />} {state.isConnected ? "End Session" : "Start Session"}
      </Button>{' '}
      <Button onClick={() => setState(p => ({ ...p, isMuted: !p.isMuted }))} variant="secondary">
        {state.isMuted ? <FaMicrophoneSlash /> : <FaMicrophone />} Mute
      </Button>
    </Container>
  );
}
